{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: inflect in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (7.5.0)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: more_itertools>=8.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from inflect) (10.5.0)\n",
      "Requirement already satisfied: typeguard>=4.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from inflect) (4.4.1)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (1.26.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from typeguard>=4.0.1->inflect) (4.12.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk inflect pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Text Preprocessing\n",
    "\n",
    "from: https://www.geeksforgeeks.org/text-preprocessing-in-python-set-1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "### Create a file called `example.txt` and write text into it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!echo \"Text preprocessing is an important step in Natural Language Processing (NLP) tasks. \\\n",
    "It involves cleaning and preparing text data for analysis or machine learning models. \\\n",
    "\\\n",
    "Some of the key steps in text preprocessing include: \\\n",
    "1 -  Converting text to lowercase \\\n",
    "2 -  Removing punctuations, numbers, and special characters \\\n",
    "3 -  Tokenization: Splitting text into individual words \\\n",
    "4 -  Removing stopwords like 'the', 'is', 'in', etc. \\\n",
    "5 -  Stemming and Lemmatization: Reducing words to their root forms \\\n",
    "\\\n",
    "These steps help standardize text data, making it easier for models to interpret and analyze.\" > example.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "#### Confirm the content was added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text preprocessing is an important step in Natural Language Processing (NLP) tasks.  It involves cleaning and preparing text data for analysis or machine learning models.   Some of the key steps in text preprocessing include:  1 -  Converting text to lowercase  2 -  Removing punctuations, numbers, and special characters  3 -  Tokenization: Splitting text into individual words  4 -  Removing stopwords like 'the', 'is', 'in', etc.  5 -  Stemming and Lemmatization: Reducing words to their root forms   These steps help standardize text data, making it easier for models to interpret and analyze.\n"
     ]
    }
   ],
   "source": [
    "!cat example.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "#### Read the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Text preprocessing is an important step in Natural Language Processing (NLP) tasks.  It involves cleaning and preparing text data for analysis or machine learning models.   Some of the key steps in text preprocessing include:  1 -  Converting text to lowercase  2 -  Removing punctuations, numbers, and special characters  3 -  Tokenization: Splitting text into individual words  4 -  Removing stopwords like 'the', 'is', 'in', etc.  5 -  Stemming and Lemmatization: Reducing words to their root forms   These steps help standardize text data, making it easier for models to interpret and analyze.\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('example.txt', 'r') as file:\n",
    "    document = file.read()\n",
    "\n",
    "document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import inflect\n",
    "import string\n",
    "import re\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Lowercase\n",
    "\n",
    "We lowercase the text to reduce the size of the vocabulary of our text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"text preprocessing is an important step in natural language processing (nlp) tasks.  it involves cleaning and preparing text data for analysis or machine learning models.   some of the key steps in text preprocessing include:  1 -  converting text to lowercase  2 -  removing punctuations, numbers, and special characters  3 -  tokenization: splitting text into individual words  4 -  removing stopwords like 'the', 'is', 'in', etc.  5 -  stemming and lemmatization: reducing words to their root forms   these steps help standardize text data, making it easier for models to interpret and analyze.\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "text_lowercase(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove numbers\n",
    "\n",
    "We can either remove numbers or convert the numbers into their textual representations. \n",
    "\n",
    "We can use regular expressions to remove the numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Text preprocessing is an important step in Natural Language Processing (NLP) tasks.  It involves cleaning and preparing text data for analysis or machine learning models.   Some of the key steps in text preprocessing include:   -  Converting text to lowercase   -  Removing punctuations, numbers, and special characters   -  Tokenization: Splitting text into individual words   -  Removing stopwords like 'the', 'is', 'in', etc.   -  Stemming and Lemmatization: Reducing words to their root forms   These steps help standardize text data, making it easier for models to interpret and analyze.\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_numbers(text):\n",
    "    result = re.sub(r'\\d+', '', text)\n",
    "    return result\n",
    "\n",
    "remove_numbers(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also convert the numbers into words. This can be done by using the `inflect` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Text preprocessing is an important step in Natural Language Processing (NLP) tasks. It involves cleaning and preparing text data for analysis or machine learning models. Some of the key steps in text preprocessing include: one - Converting text to lowercase two - Removing punctuations, numbers, and special characters three - Tokenization: Splitting text into individual words four - Removing stopwords like 'the', 'is', 'in', etc. five - Stemming and Lemmatization: Reducing words to their root forms These steps help standardize text data, making it easier for models to interpret and analyze.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the inflect library\n",
    "\n",
    "p = inflect.engine()\n",
    "\n",
    "# convert number into words\n",
    "def convert_number(text):\n",
    "    # split string into list of words\n",
    "    temp_str = text.split()\n",
    "    # initialise empty list\n",
    "    new_string = []\n",
    "\n",
    "    for word in temp_str:\n",
    "        # if word is a digit, convert the digit\n",
    "        # to numbers and append into the new_string list\n",
    "        if word.isdigit():\n",
    "            temp = p.number_to_words(word)\n",
    "            new_string.append(temp)\n",
    "\n",
    "        # append the word as it is\n",
    "        else:\n",
    "            new_string.append(word)\n",
    "\n",
    "    # join the words of new_string to form a string\n",
    "    temp_str = ' '.join(new_string)\n",
    "    return temp_str\n",
    "\n",
    "convert_number(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove punctuation\n",
    "\n",
    "We remove punctuations so that we don’t have different forms of the same word. If we don’t remove the punctuation, then been. been, been! will be treated separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Text preprocessing is an important step in Natural Language Processing NLP tasks  It involves cleaning and preparing text data for analysis or machine learning models   Some of the key steps in text preprocessing include  1   Converting text to lowercase  2   Removing punctuations numbers and special characters  3   Tokenization Splitting text into individual words  4   Removing stopwords like the is in etc  5   Stemming and Lemmatization Reducing words to their root forms   These steps help standardize text data making it easier for models to interpret and analyze\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "remove_punctuation(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove whitespace\n",
    "\n",
    "We can use the join and split function to remove all the white spaces in a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Text preprocessing is an important step in Natural Language Processing (NLP) tasks. It involves cleaning and preparing text data for analysis or machine learning models. Some of the key steps in text preprocessing include: 1 - Converting text to lowercase 2 - Removing punctuations, numbers, and special characters 3 - Tokenization: Splitting text into individual words 4 - Removing stopwords like 'the', 'is', 'in', etc. 5 - Stemming and Lemmatization: Reducing words to their root forms These steps help standardize text data, making it easier for models to interpret and analyze.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_whitespace(text):\n",
    "    return  \" \".join(text.split())\n",
    "\n",
    "remove_whitespace(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download required resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/mehdi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /Users/mehdi/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/mehdi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/mehdi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove default stopwords\n",
    "\n",
    "Stopwords are words that do not contribute to the meaning of a sentence. Hence, they can safely be removed without causing any change in the meaning of the sentence. The NLTK library has a set of stopwords and we can use these to remove stopwords from our text and return a list of word tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Text',\n",
       " 'preprocessing',\n",
       " 'important',\n",
       " 'step',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " '(',\n",
       " 'NLP',\n",
       " ')',\n",
       " 'tasks',\n",
       " '.',\n",
       " 'It',\n",
       " 'involves',\n",
       " 'cleaning',\n",
       " 'preparing',\n",
       " 'text',\n",
       " 'data',\n",
       " 'analysis',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'models',\n",
       " '.',\n",
       " 'Some',\n",
       " 'key',\n",
       " 'steps',\n",
       " 'text',\n",
       " 'preprocessing',\n",
       " 'include',\n",
       " ':',\n",
       " '1',\n",
       " '-',\n",
       " 'Converting',\n",
       " 'text',\n",
       " 'lowercase',\n",
       " '2',\n",
       " '-',\n",
       " 'Removing',\n",
       " 'punctuations',\n",
       " ',',\n",
       " 'numbers',\n",
       " ',',\n",
       " 'special',\n",
       " 'characters',\n",
       " '3',\n",
       " '-',\n",
       " 'Tokenization',\n",
       " ':',\n",
       " 'Splitting',\n",
       " 'text',\n",
       " 'individual',\n",
       " 'words',\n",
       " '4',\n",
       " '-',\n",
       " 'Removing',\n",
       " 'stopwords',\n",
       " 'like',\n",
       " \"'the\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'is\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'in\",\n",
       " \"'\",\n",
       " ',',\n",
       " 'etc',\n",
       " '.',\n",
       " '5',\n",
       " '-',\n",
       " 'Stemming',\n",
       " 'Lemmatization',\n",
       " ':',\n",
       " 'Reducing',\n",
       " 'words',\n",
       " 'root',\n",
       " 'forms',\n",
       " 'These',\n",
       " 'steps',\n",
       " 'help',\n",
       " 'standardize',\n",
       " 'text',\n",
       " 'data',\n",
       " ',',\n",
       " 'making',\n",
       " 'easier',\n",
       " 'models',\n",
       " 'interpret',\n",
       " 'analyze',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return filtered_text\n",
    "\n",
    "remove_stopwords(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "Stemming is the process of getting the root form of a word. Stem or root is the part to which inflectional affixes (-ed, -ize, -de, -s, etc.) are added. The stem of a word is created by removing the prefix or suffix of a word. So, stemming a word may not result in actual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text',\n",
       " 'preprocess',\n",
       " 'is',\n",
       " 'an',\n",
       " 'import',\n",
       " 'step',\n",
       " 'in',\n",
       " 'natur',\n",
       " 'languag',\n",
       " 'process',\n",
       " '(',\n",
       " 'nlp',\n",
       " ')',\n",
       " 'task',\n",
       " '.',\n",
       " 'it',\n",
       " 'involv',\n",
       " 'clean',\n",
       " 'and',\n",
       " 'prepar',\n",
       " 'text',\n",
       " 'data',\n",
       " 'for',\n",
       " 'analysi',\n",
       " 'or',\n",
       " 'machin',\n",
       " 'learn',\n",
       " 'model',\n",
       " '.',\n",
       " 'some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'key',\n",
       " 'step',\n",
       " 'in',\n",
       " 'text',\n",
       " 'preprocess',\n",
       " 'includ',\n",
       " ':',\n",
       " '1',\n",
       " '-',\n",
       " 'convert',\n",
       " 'text',\n",
       " 'to',\n",
       " 'lowercas',\n",
       " '2',\n",
       " '-',\n",
       " 'remov',\n",
       " 'punctuat',\n",
       " ',',\n",
       " 'number',\n",
       " ',',\n",
       " 'and',\n",
       " 'special',\n",
       " 'charact',\n",
       " '3',\n",
       " '-',\n",
       " 'token',\n",
       " ':',\n",
       " 'split',\n",
       " 'text',\n",
       " 'into',\n",
       " 'individu',\n",
       " 'word',\n",
       " '4',\n",
       " '-',\n",
       " 'remov',\n",
       " 'stopword',\n",
       " 'like',\n",
       " \"'the\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'i\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'in\",\n",
       " \"'\",\n",
       " ',',\n",
       " 'etc',\n",
       " '.',\n",
       " '5',\n",
       " '-',\n",
       " 'stem',\n",
       " 'and',\n",
       " 'lemmat',\n",
       " ':',\n",
       " 'reduc',\n",
       " 'word',\n",
       " 'to',\n",
       " 'their',\n",
       " 'root',\n",
       " 'form',\n",
       " 'these',\n",
       " 'step',\n",
       " 'help',\n",
       " 'standard',\n",
       " 'text',\n",
       " 'data',\n",
       " ',',\n",
       " 'make',\n",
       " 'it',\n",
       " 'easier',\n",
       " 'for',\n",
       " 'model',\n",
       " 'to',\n",
       " 'interpret',\n",
       " 'and',\n",
       " 'analyz',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_words(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    stems = [stemmer.stem(word) for word in word_tokens]\n",
    "    return stems\n",
    "\n",
    "stem_words(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "Lemmatization is a natural language processing (NLP) technique that reduces a word to its root form. This can be helpful for tasks such as text analysis and search, as it allows you to compare words that are related but have different forms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Text',\n",
       " 'preprocessing',\n",
       " 'is',\n",
       " 'an',\n",
       " 'important',\n",
       " 'step',\n",
       " 'in',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " '(',\n",
       " 'NLP',\n",
       " ')',\n",
       " 'task',\n",
       " '.',\n",
       " 'It',\n",
       " 'involves',\n",
       " 'cleaning',\n",
       " 'and',\n",
       " 'preparing',\n",
       " 'text',\n",
       " 'data',\n",
       " 'for',\n",
       " 'analysis',\n",
       " 'or',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'model',\n",
       " '.',\n",
       " 'Some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'key',\n",
       " 'step',\n",
       " 'in',\n",
       " 'text',\n",
       " 'preprocessing',\n",
       " 'include',\n",
       " ':',\n",
       " '1',\n",
       " '-',\n",
       " 'Converting',\n",
       " 'text',\n",
       " 'to',\n",
       " 'lowercase',\n",
       " '2',\n",
       " '-',\n",
       " 'Removing',\n",
       " 'punctuation',\n",
       " ',',\n",
       " 'number',\n",
       " ',',\n",
       " 'and',\n",
       " 'special',\n",
       " 'character',\n",
       " '3',\n",
       " '-',\n",
       " 'Tokenization',\n",
       " ':',\n",
       " 'Splitting',\n",
       " 'text',\n",
       " 'into',\n",
       " 'individual',\n",
       " 'word',\n",
       " '4',\n",
       " '-',\n",
       " 'Removing',\n",
       " 'stopwords',\n",
       " 'like',\n",
       " \"'the\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'is\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'in\",\n",
       " \"'\",\n",
       " ',',\n",
       " 'etc',\n",
       " '.',\n",
       " '5',\n",
       " '-',\n",
       " 'Stemming',\n",
       " 'and',\n",
       " 'Lemmatization',\n",
       " ':',\n",
       " 'Reducing',\n",
       " 'word',\n",
       " 'to',\n",
       " 'their',\n",
       " 'root',\n",
       " 'form',\n",
       " 'These',\n",
       " 'step',\n",
       " 'help',\n",
       " 'standardize',\n",
       " 'text',\n",
       " 'data',\n",
       " ',',\n",
       " 'making',\n",
       " 'it',\n",
       " 'easier',\n",
       " 'for',\n",
       " 'model',\n",
       " 'to',\n",
       " 'interpret',\n",
       " 'and',\n",
       " 'analyze',\n",
       " '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemma_words(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    lemmas = [lemmatizer.lemmatize(word) for word in word_tokens]\n",
    "    return lemmas\n",
    "\n",
    "lemma_words(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Inverted Index\n",
    "\n",
    "from: https://www.geeksforgeeks.org/inverted-index/\n",
    "\n",
    "Define the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "document1 = \"The quick brown fox jumped over the lazy dog.\"\n",
    "document2 = \"The lazy dog slept in the sun.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the documents\n",
    "\n",
    "Convert each document to lowercase and split it into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens1 = document1.lower().split()\n",
    "tokens2 = document2.lower().split()\n",
    "\n",
    "# Combine the tokens into a list of unique terms\n",
    "terms = list(set(tokens1 + tokens2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fox -> Document 1\n",
      "the -> Document 1, Document 2\n",
      "dog -> Document 2\n",
      "slept -> Document 2\n",
      "dog. -> Document 1\n",
      "quick -> Document 1\n",
      "over -> Document 1\n",
      "sun. -> Document 2\n",
      "jumped -> Document 1\n",
      "lazy -> Document 1, Document 2\n",
      "in -> Document 2\n",
      "brown -> Document 1\n"
     ]
    }
   ],
   "source": [
    "# Create an empty dictionary to store the inverted index\n",
    "inverted_index = {}\n",
    "\n",
    "# For each term, find the documents that contain it\n",
    "for term in terms:\n",
    "\tdocuments = []\n",
    "\tif term in tokens1:\n",
    "\t\tdocuments.append(\"Document 1\")\n",
    "\tif term in tokens2:\n",
    "\t\tdocuments.append(\"Document 2\")\n",
    "\tinverted_index[term] = documents\n",
    "\n",
    "for term, documents in inverted_index.items():\n",
    "\tprint(term, \"->\", \", \".join(documents))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Document Retrieval using Boolean Model\n",
    "\n",
    "from: https://www.geeksforgeeks.org/document-retrieval-using-boolean-model-and-vector-space-model/\n",
    "\n",
    "Create documents CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "echo \"documents,term1,term2,term3\n",
    "document1,\\\"ice cream\\\",mango,litchi\n",
    "document2,hockey,cricket,sport\n",
    "document3,litchi,mango,chocolate\n",
    "document4,nice,good,cute\" > documents.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "def filter(documents, rows, cols):\n",
    "    '''function to read and separate the name of the documents and the terms \n",
    "    present in it to a separate list  from the data frame and also create a \n",
    "    dictionary which has the name of the document as key and the terms present in\n",
    "    it as the list of strings  which is the value of the key'''\n",
    "\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            # traversal through the data frame\n",
    "\n",
    "            if(j == 0):\n",
    "                # first column has the name of the document in the csv file\n",
    "                keys.append(documents.loc[i].iat[j])\n",
    "            else:\n",
    "                dummy_List.append(documents.loc[i].iat[j])\n",
    "                # dummy list to update the terms in the dictionary\n",
    "\n",
    "                if documents.loc[i].iat[j] not in terms:\n",
    "                    # add the terms to the list if it is not present else continue\n",
    "                    terms.append(documents.loc[i].iat[j])\n",
    "\n",
    "        copy = dummy_List.copy()\n",
    "        # copying the dummy list to a different list\n",
    "\n",
    "        dicti.update({documents.loc[i].iat[0]: copy})\n",
    "        # adding the key value pair to a dictionary\n",
    "\n",
    "        dummy_List.clear()\n",
    "        # clearing the dummy list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "def bool_Representation(dicti, rows, cols):\n",
    "    '''In this function we get a boolean representation of the terms present in the\n",
    "    documents in the form of lists, later we create a dictionary which contains \n",
    "    the name of the documents as key and value as the list of boolean values \n",
    "    representing the terms present in the document'''\n",
    "\n",
    "    terms.sort()\n",
    "    # we sort the elements in the alphabetical order for the convience, the order\n",
    "    # of the term does not make any difference\n",
    "\n",
    "    for i in (dicti):\n",
    "        # for every document in the dictionary we check for each string present in\n",
    "        # the list\n",
    "\n",
    "        for j in terms:\n",
    "            # if the string is present in the list we append 1 else we append 0\n",
    "\n",
    "            if j in dicti[i]:\n",
    "                dummy_List.append(1)\n",
    "            else:\n",
    "                dummy_List.append(0)\n",
    "            # appending 1 or 0 for obtaining the boolean representation\n",
    "\n",
    "        copy = dummy_List.copy()\n",
    "        # copying the dummy list to a different list\n",
    "\n",
    "        vec_Dic.update({i: copy})\n",
    "        # adding the key value pair to a dictionary\n",
    "\n",
    "        dummy_List.clear()\n",
    "        # clearing the dummy list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "def query_Vector(query):\n",
    "    '''In this function we represent the query in the form of boolean vector'''\n",
    "\n",
    "    qvect = []\n",
    "    # query vector which is returned at the end of the function\n",
    "\n",
    "    for i in terms:\n",
    "        # if the word present in the list of terms is also present in the query\n",
    "        # then append 1 else append 0\n",
    "\n",
    "        if i in query:\n",
    "            qvect.append(1)\n",
    "        else:\n",
    "            qvect.append(0)\n",
    "\n",
    "    return qvect\n",
    "    # return the query vector which is obtained in the boolean form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "def prediction(q_Vect):\n",
    "    '''In this function we make the prediction regarding which document is related\n",
    "    to the given query by performing the boolean operations'''\n",
    "\n",
    "    dictionary = {}\n",
    "    listi = []\n",
    "    count = 0\n",
    "    # initialisation of the dictionary , list and a variable which is further\n",
    "    # required for performing the computation\n",
    "\n",
    "    term_Len = len(terms)\n",
    "    # number of terms present in the term list\n",
    "\n",
    "    for i in vec_Dic:\n",
    "        # for every document in the dictionary containing the terms present in it\n",
    "        # the form of boolean vector\n",
    "\n",
    "        for t in range(term_Len):\n",
    "            if(q_Vect[t] == vec_Dic[i][t]):\n",
    "                # if the words present in the query is also present in the\n",
    "                # document or if the words present in the query is also absent in\n",
    "                # the document\n",
    "\n",
    "                count += 1\n",
    "                # increase the value of count variable by one\n",
    "                # the condition in which words present in document and absent in\n",
    "                #query , present in query and absent in document is not considered\n",
    "\n",
    "        dictionary.update({i: count})\n",
    "        # dictionary updation here the name of the document is the key and the\n",
    "        # count variable computed earlier is the value\n",
    "\n",
    "        count = 0\n",
    "        # reinitialisaion of count variable to 0\n",
    "\n",
    "    for i in dictionary:\n",
    "        listi.append(dictionary[i])\n",
    "        # here we append the count value to list\n",
    "\n",
    "    listi = sorted(listi, reverse=True)\n",
    "    # we sort the list in the descending order which is needed to rank the \n",
    "    #documents according to the relevance\n",
    "\n",
    "    ans = ' '\n",
    "    # variable to store the name of the document which is most relevant\n",
    "    \n",
    "    with open('output.txt', 'w') as f:\n",
    "        \n",
    "        with redirect_stdout(f):\n",
    "            # to redirect the output to a text file\n",
    "            print(\"ranking of the documents\")\n",
    "\n",
    "            for count, i in enumerate(listi):\n",
    "                key = check(dictionary, i)\n",
    "                # Function call to get the key when the value is known\n",
    "                if count == 0:\n",
    "                    ans = key\n",
    "                    # to store the name of the document which is most relevant\n",
    "\n",
    "                print(key, \"rank is\", count+1)\n",
    "                # print the name of the document along with its rank\n",
    "\n",
    "                dictionary.pop(key)\n",
    "                # remove the key from the dictionary after printing\n",
    "\n",
    "            print(ans, \"is the most relevant document for the given query\")\n",
    "            # to print the name of the document which is most relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "def check(dictionary, val):\n",
    "    '''Function to return the key when the value is known'''\n",
    "\n",
    "    for key, value in dictionary.items():\n",
    "        if(val == value):\n",
    "            # if the given value is same as the value present in the dictionary\n",
    "            # return the key\n",
    "\n",
    "            return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter query:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hockey is a national sport\n"
     ]
    }
   ],
   "source": [
    "# module to redirect the output to a text file\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "# list to store the terms present in the documents\n",
    "terms = []\n",
    "\n",
    "# list to store the names of the documents\n",
    "keys = []\n",
    "\n",
    "# dictionary to store the name of the document and the boolean vector as list\n",
    "vec_Dic = {}\n",
    "\n",
    "# dictionary to store the name of the document and the terms present in it as a vector\n",
    "dicti = {}\n",
    "\n",
    "# list for performing some operations and clearing them\n",
    "dummy_List = []\n",
    "\n",
    "documents = pandas.read_csv(r'documents.csv')\n",
    "# to read the data from the csv file as a dataframe\n",
    "rows = len(documents)\n",
    "# to get the number of rows\n",
    "cols = len(documents.columns)\n",
    "# to get the number of columns\n",
    "filter(documents, rows, cols)\n",
    "# function call to read and separate the name of the documents and the terms\n",
    "# present in it to a separate list  from the data frame and also create a\n",
    "# dictionary which has the name of the document as key and the terms present in\n",
    "# it as the list of strings  which is the value of the key\n",
    "bool_Representation(dicti, rows, cols)\n",
    "# In this function we get a boolean representation of the terms present in the\n",
    "# documents in the form of lists, later we create a dictionary which contains\n",
    "# the name of the documents as key and value as the list of boolean values\n",
    "#representing the terms present in the document\n",
    "print(\"Enter query:\")\n",
    "query = input()\n",
    "\n",
    "print(query)\n",
    "\n",
    "# to get the query input from the user, the below input is given for obtaining\n",
    "# the output as in output.txt file\n",
    "# hockey is a national sport\n",
    "query = query.split(' ')\n",
    "# splitting the query as a list of strings\n",
    "q_Vect = query_Vector(query)\n",
    "# function call to represent the query in the form of boolean vector\n",
    "prediction(q_Vect)\n",
    "# Function call to make the prediction regarding which document is related to\n",
    "# the given query by performing the boolean operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ranking of the documents\n",
      "document2 rank is 1\n",
      "document1 rank is 2\n",
      "document3 rank is 3\n",
      "document4 rank is 4\n",
      "document2 is the most relevant document for the given query\n"
     ]
    }
   ],
   "source": [
    "!cat output.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
